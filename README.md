# Qwen Image Edit 3. **‚ùå NOT USING VENV**: Do NOT install in system Python
   - ‚úÖ USE: Always activate `.venv` before running ANY command
   - Check prompt shows `(.venv)` at the start
   - If you see errors, you're probably not in the venv

4. **‚ùå WRONG PARAMETERS**: Do NOT assume all models use the same parameters
   - ‚úÖ ALWAYS check the HuggingFace model card for optimal settings
   - Lightning/Turbo/distilled models need different `true_cfg_scale` values
   - **Example**: Standard models use `true_cfg_scale=4.0`, Lightning uses `1.0`
   - Using wrong parameters = poor quality, blocky/pixelated output
   - See "Model-Specific Parameters" section below

## üéØ Overview
AI-powered multi-image editing using Qwen's Image Edit model with quantized transformers for 24GB VRAM GPUs.

## ‚ö†Ô∏è CRITICAL: READ THIS FIRST

### üö´ Three Common Mistakes That Will Waste Hours

1. **‚ùå WRONG MODEL**: Do NOT use `Qwen/Qwen-Image-Edit-2509` (40GB full model)
   - ‚úÖ USE: `nunchaku-tech/nunchaku-qwen-image-edit-2509` (12.7GB quantized)
   - The 40GB model **WILL NOT FIT** on RTX 4090 24GB VRAM
   - You will get OOM (Out of Memory) errors

2. **‚ùå WRONG NUNCHAKU**: Do NOT use `pip install nunchaku` (0.15.4 stats package)
   - ‚úÖ USE: Install from source (see below)
   - The PyPI package "nunchaku" is a completely different stats library
   - You need `nunchaku==1.0.1+torch2.5` for AI model quantization

3. **‚ùå NOT USING VENV**: Do NOT install in system Python
   - ‚úÖ ALWAYS activate `.venv` before running ANY command
   - Check prompt shows `(.venv)` at the start
   - If you see errors, you're probably not in the venv

## üéØ Overview

This project implements the Qwen Image Edit 2509 model using quantized INT4 transformers via nunchaku, enabling high-quality multi-image AI editing on consumer GPUs like the RTX 4090 (24GB VRAM).

**Two scripts available:**
- `qwen_image_edit_nunchaku.py` - Standard 40-step model (best quality, ~2:45)
- `qwen_image_edit_lightning.py` - Lightning 8-step model (fast, ~21s)

## ‚ú® Features

- **Quantized Model Support**: Uses INT4 quantization (rank 128) to fit in 24GB VRAM
- **Multi-Image Editing**: Combine and edit multiple images with AI guidance
- **High Quality Output**: ~12.7GB quantized model maintains excellent quality
- **Lightning Fast Option**: 8-step model generates in ~21 seconds (7.7x faster!)
- **CUDA-Optimized**: Built for NVIDIA GPUs with Compute Capability 8.9 (RTX 4090)

## üñºÔ∏è Example

**Prompt**: "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."

**Generated Image**: `output_image_edit_plus_r128.png`
- **Inference Time**: 2:44 (40 steps)
- **Model Size**: 12.7GB quantized
- **VRAM Usage**: ~23GB

## üõ†Ô∏è System Requirements

### Hardware
- **GPU**: NVIDIA GeForce RTX 4090 (24GB VRAM) or similar
- **VRAM**: 24GB minimum
- **Disk Space**: ~50GB for models and dependencies
- **RAM**: 32GB recommended
- **Compute Capability**: 8.9 (sm_89)

### Software
- **OS**: Windows 10/11
- **Python**: 3.10.6
- **CUDA**: 13.0 (or 12.1+)
- **Driver**: 581.29 or newer
- **Visual Studio Build Tools 2022**: With C++ components

## üì¶ Installation

### Step 1: Create Virtual Environment

**‚ö†Ô∏è CRITICAL: You MUST use a virtual environment!**

```powershell
# Navigate to project directory
cd C:\Projects\qwen-image-edit

# Create virtual environment
python -m venv .venv

# Activate it (DO THIS EVERY TIME)
.\.venv\Scripts\Activate.ps1

# Verify activation - you should see (.venv) in your prompt
# Your prompt should look like: (.venv) PS C:\Projects\qwen-image-edit>
```

### Step 2: Install PyTorch with CUDA

```powershell
# MAKE SURE (.venv) is showing in your prompt!
pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121
```

### Step 3: Install Diffusers from GitHub

**‚ö†Ô∏è Must be from GitHub for QwenImageEditPlusPipeline support**

```powershell
# MAKE SURE (.venv) is showing in your prompt!
pip install git+https://github.com/huggingface/diffusers.git
```

### Step 4: Install Other Dependencies

```powershell
# MAKE SURE (.venv) is showing in your prompt!
pip install -r requirements.txt
```

### Step 5: Install nunchaku from Source

**‚ö†Ô∏è DO NOT use `pip install nunchaku` - that's a different package!**

See [INSTALL_NUNCHAKU.md](INSTALL_NUNCHAKU.md) for complete instructions.

**Quick version:**
```powershell
# MAKE SURE (.venv) is showing in your prompt!

# Install Visual Studio Build Tools 2022 first (if not already installed)
# Download from: https://visualstudio.microsoft.com/downloads/

# Clone and build nunchaku
cd $env:TEMP
git clone https://github.com/nunchaku-tech/nunchaku.git
cd nunchaku
git submodule update --init --recursive
$env:TORCH_CUDA_ARCH_LIST="8.9"
$env:DISTUTILS_USE_SDK="1"
pip install -e . --no-build-isolation

# Return to project
cd C:\Projects\qwen-image-edit
```

**Verify nunchaku installation:**
```powershell
# Should show: nunchaku==1.0.1+torch2.5 (NOT 0.15.4!)
pip show nunchaku
```

### Step 6: Run Prerequisites Check

```powershell
# MAKE SURE (.venv) is showing in your prompt!
.\check.ps1
```

## üöÄ Quick Start

**‚ö†Ô∏è ALWAYS activate venv first!**

```powershell
# Navigate to project
cd C:\Projects\qwen-image-edit

# Activate virtual environment (DO THIS EVERY TIME!)
.\.venv\Scripts\Activate.ps1

# Verify you see (.venv) in your prompt
# Prompt should show: (.venv) PS C:\Projects\qwen-image-edit>

# Option 1: Standard model (best quality, slower)
python qwen_image_edit_nunchaku.py

# Option 2: Lightning model (fast, very good quality)
python qwen_image_edit_lightning.py
```

The script will:
1. Download the quantized model (~12.7GB) on first run
2. Download sample images from Qwen examples
3. Generate an edited image combining both inputs
4. Save output to `generated-images/[output|lightning]_r128_YYYYMMDD_HHMMSS.png`

**Generation Time**: 
- Standard: ~2:45 (40 steps)
- Lightning: ~21s (8 steps) ‚ö° **7.7x faster!**

**Output**: All generated images are saved in the `generated-images/` folder with timestamps to prevent overwriting.

## üìä Performance

- **First Run**: ~5 minutes (model download + generation)
- **Subsequent Runs**: ~2-3 minutes (generation only)
- **VRAM Usage**: ~23GB during inference
- **Model Download Size**: 12.7GB quantized model
- **Inference Steps**: 40 (adjustable: 20-50)

## üìÅ Project Structure

```
qwen-image-edit/
‚îú‚îÄ‚îÄ .venv/                          # Virtual environment (do not commit)
‚îú‚îÄ‚îÄ generated-images/               # Generated images output folder
‚îú‚îÄ‚îÄ qwen_image_edit_nunchaku.py    # Main script
‚îú‚îÄ‚îÄ check.ps1                       # Prerequisites checker
‚îú‚îÄ‚îÄ install-nunchaku-patched.ps1   # Installation helper
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ TODO.txt                        # TODO list and improvements
‚îî‚îÄ‚îÄ .gitignore                      # Git ignore rules
```

## üîß Model Options

### ‚úÖ Quantized Models (THE ONLY MODELS THAT WORK on RTX 4090)

**‚ö†Ô∏è IMPORTANT: Only use models from `nunchaku-tech/nunchaku-qwen-image-edit-2509`**

**Standard Models (40 steps):**
- `svdq-int4_r32` (11.5 GB) - Good quality
- `svdq-int4_r128` (12.7 GB) ‚≠ê **Best Quality** ‚Üê **WE USE THIS ONE**

**Lightning Models (8 steps - Faster):**
- `svdq-int4_r32-lightningv2.0-8steps` - Fast
- `svdq-int4_r128-lightningv2.0-8steps` ‚≠ê **Best Balance** - Fast + Quality

**Lightning Models (4 steps - Fastest):**
- `svdq-int4_r32-lightningv2.0-4steps` - Very fast
- `svdq-int4_r128-lightningv2.0-4steps` - Very fast + Better quality

**Current script uses:** `nunchaku-tech/nunchaku-qwen-image-edit-2509/svdq-int4_r128`

## üéõÔ∏è Model-Specific Parameters

**‚ö†Ô∏è CRITICAL: Different models require different parameters!**

### Standard Models (40 steps)
```python
inputs = {
    "num_inference_steps": 40,
    "true_cfg_scale": 4.0,      # High guidance for standard models
    "guidance_scale": 1.0,
    "negative_prompt": " ",
}
```
- **Quality**: Best
- **Speed**: Slow (~2:45 for 40 steps)
- **Use case**: Final production quality

### Lightning Models (8 steps)
```python
inputs = {
    "num_inference_steps": 8,
    "true_cfg_scale": 1.0,       # ‚ö†Ô∏è DIFFERENT! Lightning uses 1.0
    "guidance_scale": 1.0,
    "negative_prompt": " ",
}
```
- **Quality**: Very Good
- **Speed**: Fast (~21 seconds for 8 steps)
- **Use case**: Quick iterations, testing prompts

### Lightning Models (4 steps)
```python
inputs = {
    "num_inference_steps": 4,
    "true_cfg_scale": 1.0,       # Same as 8-step
    "guidance_scale": 1.0,
    "negative_prompt": " ",
}
```
- **Quality**: Good
- **Speed**: Very Fast (~10 seconds for 4 steps)
- **Use case**: Rapid prototyping

### üîç How to Find Optimal Parameters

**ALWAYS check the HuggingFace model card before using a new model!**

1. **Visit the model repository:**
   - Standard: https://huggingface.co/Qwen/Qwen-Image-Edit-2509
   - Lightning: https://huggingface.co/lightx2v/Qwen-Image-Lightning
   - Quantized: https://huggingface.co/nunchaku-tech/nunchaku-qwen-image-edit-2509

2. **Look for example code:**
   - Check the "Model card" tab
   - Look for usage examples with `DiffusionPipeline` or similar
   - Note the values for:
     - `num_inference_steps`
     - `true_cfg_scale` ‚ö†Ô∏è **Most important!**
     - `guidance_scale`

3. **Common mistakes:**
   - ‚ùå Using `true_cfg_scale=4.0` with Lightning ‚Üí Blocky, pixelated output
   - ‚ùå Using wrong number of steps ‚Üí Poor quality or wasted time
   - ‚ùå Assuming all models use same parameters ‚Üí Unpredictable results

### üìã Quick Reference Table

| Model Type | Steps | true_cfg_scale | Time | Quality |
|------------|-------|----------------|------|---------|
| Standard r128 | 40 | 4.0 | ~2:45 | Best |
| Lightning 8-step r128 | 8 | 1.0 | ~21s | Very Good |
| Lightning 4-step r128 | 4 | 1.0 | ~10s | Good |
| Standard r32 | 40 | 4.0 | ~2:00 | Good |
| Lightning 8-step r32 | 8 | 1.0 | ~18s | Good |

### ‚ùå DO NOT USE: Full Model (Will Crash!)

**üö´ `Qwen/Qwen-Image-Edit-2509` (~40GB) - DO NOT USE!**

This is the WRONG model. It will:
- ‚ùå Cause OOM (Out of Memory) errors
- ‚ùå Crash your system
- ‚ùå NOT fit on RTX 4090 24GB VRAM
- ‚ùå Waste hours of your time downloading it

**IF YOU SEE THIS IN YOUR CODE, YOU'RE USING THE WRONG MODEL:**
```python
# ‚ùå WRONG - DO NOT USE
pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509")
```

**‚úÖ CORRECT - USE THIS:**
```python
# ‚úÖ CORRECT - Load quantized transformer first
transformer = NunchakuQwenImageTransformer2DModel.from_pretrained(
    "nunchaku-tech/nunchaku-qwen-image-edit-2509/svdq-int4_r128-qwen-image-edit-2509.safetensors"
)
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2509",  # Pipeline config only
    transformer=transformer  # Use quantized transformer
)
```

## ÔøΩ How to Verify Everything is Correct

### ‚úÖ Check 1: Virtual Environment is Active

```powershell
# Your prompt should look like this:
# (.venv) PS C:\Projects\qwen-image-edit>

# If it doesn't, activate venv:
.\.venv\Scripts\Activate.ps1
```

### ‚úÖ Check 2: Correct nunchaku Version

```powershell
# MAKE SURE (.venv) is showing in your prompt!
pip show nunchaku

# Should show:
# Name: nunchaku
# Version: 1.0.1+torch2.5
# Location: C:\Users\...\AppData\Local\Temp\nunchaku

# ‚ùå If it shows Version: 0.15.4 - YOU HAVE THE WRONG PACKAGE!
# Fix: pip uninstall nunchaku -y
# Then follow nunchaku installation steps above
```

### ‚úÖ Check 3: Correct Model in Code

```powershell
# Check your script uses quantized models
Get-Content qwen_image_edit_nunchaku.py | Select-String "nunchaku-tech"

# Should show:
# "nunchaku-tech/nunchaku-qwen-image-edit-2509/svdq-..."

# ‚ùå If it shows only "Qwen/Qwen-Image-Edit-2509" without nunchaku-tech
# YOU'RE USING THE WRONG 40GB MODEL!
```

### ‚úÖ Check 4: All Dependencies Installed

```powershell
# MAKE SURE (.venv) is showing in your prompt!
.\check.ps1

# Should show all ‚úì checks passing
```

## üõ†Ô∏è Detailed Installation Guide

See [INSTALL_NUNCHAKU.md](INSTALL_NUNCHAKU.md) for complete step-by-step instructions including:
- Visual Studio Build Tools setup
- PyTorch CUDA patch (if needed)
- Nunchaku compilation from source
- Troubleshooting common issues

## ‚ö†Ô∏è Known Issues

1. **Xet Storage Warning**: Install `hf_xet` for faster downloads:
   ```powershell
   pip install hf_xet
   ```

2. **CUDA Version Mismatch**: PyTorch compiled with CUDA 12.1 but system has CUDA 13.0
   - **Solution**: Applied patch to skip version check (see INSTALL_NUNCHAKU.md)

3. **torch_dtype Deprecation**: Minor deprecation warning
   - **Impact**: None, will be fixed in future update

4. **Config Attributes Warning**: Benign warning about `pooled_projection_dim`
   - **Impact**: None, can be ignored

## üôè Credits

- **Model**: [Qwen Image Edit 2509](https://huggingface.co/Qwen/Qwen-Image-Edit-2509) by Alibaba Cloud
- **Quantization**: [nunchaku](https://github.com/nunchaku-tech/nunchaku) by nunchaku-tech
- **Diffusers**: [Hugging Face Diffusers](https://github.com/huggingface/diffusers)

## üìù License

This project uses models and libraries with their respective licenses. Please check individual component licenses before commercial use.

## ü§ù Contributing

Contributions welcome! Please see [TODO.txt](TODO.txt) for current improvement ideas.

### For Developers: Check for Absolute Paths

Before committing changes, ensure no absolute paths exist:

```powershell
# Check all Python and PowerShell files for absolute paths
Get-ChildItem -Recurse -Include *.py,*.ps1,*.md | Select-String -Pattern "C:\\"
```

## üêõ Troubleshooting

### ‚ùå Issue: "ModuleNotFoundError: No module named 'nunchaku'"

**Cause**: One of two problems:
1. Not in virtual environment
2. Wrong nunchaku installed (stats package)

**Solution**:
```powershell
# 1. Activate venv (look for (.venv) in prompt)
.\.venv\Scripts\Activate.ps1

# 2. Check nunchaku version
pip show nunchaku

# If version is 0.15.4 - WRONG PACKAGE!
pip uninstall nunchaku -y

# Install correct nunchaku from source
# See INSTALL_NUNCHAKU.md for full instructions
```

### ‚ùå Issue: "CUDA out of memory"

**Cause**: One of three problems:
1. Using wrong 40GB full model
2. Not enough VRAM
3. Other apps using GPU

**Solution**:
```powershell
# 1. Check you're using quantized model
Get-Content qwen_image_edit_nunchaku.py | Select-String "nunchaku-tech"
# Should show: nunchaku-tech/nunchaku-qwen-image-edit-2509

# 2. Close other GPU applications
# - Close Chrome/Edge (uses GPU)
# - Close other AI apps
# - Check GPU usage: nvidia-smi

# 3. If still failing, you may be using the wrong 40GB model!
```

### ‚ùå Issue: Compilation fails during nunchaku installation

**Solution**: Install Visual Studio Build Tools 2022 with C++ components
```powershell
# Download from: https://visualstudio.microsoft.com/downloads/
# Select: "Desktop development with C++"
# See INSTALL_NUNCHAKU.md for detailed instructions
```

### ‚ùå Issue: Output image is blocky, pixelated, or low quality

**Cause**: Using wrong parameters for the model type!

**This happens when:**
- Using `true_cfg_scale=4.0` with Lightning models (should be 1.0)
- Using wrong number of inference steps
- Not checking the HuggingFace model card for optimal parameters

**Solution**:
```powershell
# 1. Check which model you're using
Get-Content your_script.py | Select-String "lightning"

# 2. If using Lightning model, check true_cfg_scale
Get-Content your_script.py | Select-String "true_cfg_scale"

# Should show:
# Lightning models: true_cfg_scale: 1.0  ‚úÖ
# Standard models:  true_cfg_scale: 4.0  ‚úÖ

# 3. Fix your script if needed:
# Lightning (8-step): true_cfg_scale=1.0, num_inference_steps=8
# Lightning (4-step): true_cfg_scale=1.0, num_inference_steps=4
# Standard (40-step): true_cfg_scale=4.0, num_inference_steps=40

# 4. ALWAYS check HuggingFace model card for new models:
#    https://huggingface.co/<model_name>
#    Look for example code with optimal parameters
```

**Prevention**: Before using any new model variant:
1. Visit the HuggingFace model page
2. Check the README for usage examples
3. Copy the exact parameters shown in examples
4. See "Model-Specific Parameters" section above

### ‚ùå Issue: Wrong nunchaku package installed (0.15.4 stats package)

**This is the #1 most common mistake!**

**Solution**:
```powershell
# MAKE SURE (.venv) is showing in your prompt!

# Remove wrong package
pip uninstall nunchaku -y

# Install correct version from source
# See INSTALL_NUNCHAKU.md for full instructions
cd $env:TEMP
git clone https://github.com/nunchaku-tech/nunchaku.git
cd nunchaku
git submodule update --init --recursive
pip install -e . --no-build-isolation
```

### ‚ùå Issue: Script runs but generates garbage/black images

**Cause**: Probably using wrong 40GB model or wrong nunchaku

**Solution**:
```powershell
# Verify BOTH are correct:

# 1. Check nunchaku version (should be 1.0.1+torch2.5)
pip show nunchaku

# 2. Check model in code (should have nunchaku-tech)
Get-Content qwen_image_edit_nunchaku.py | Select-String "nunchaku-tech"
```

### ‚ùå Issue: Commands fail with "python: command not found"

**Cause**: Not in virtual environment

**Solution**:
```powershell
# Activate venv - look for (.venv) in prompt
.\.venv\Scripts\Activate.ps1

# Verify activation worked
python --version
# Should show: Python 3.10.6
```

---

## üéØ Final Pre-Flight Checklist

**Before running the script, verify ALL of these:**

- [ ] ‚úÖ Prompt shows `(.venv)` at the start
- [ ] ‚úÖ `pip show nunchaku` shows version `1.0.1+torch2.5` (NOT 0.15.4)
- [ ] ‚úÖ `Get-Content qwen_image_edit_nunchaku.py | Select-String "nunchaku-tech"` shows quantized model path
- [ ] ‚úÖ `.\check.ps1` passes all checks
- [ ] ‚úÖ `nvidia-smi` shows RTX 4090 with 24GB VRAM available
- [ ] ‚úÖ You have ~50GB free disk space for model downloads
- [ ] ‚úÖ No other applications are using significant GPU memory

**If ANY of these are ‚ùå, fix them first before running the script!**

---

## üìö Quick Reference

**Activate venv (do this EVERY time):**
```powershell
cd C:\Projects\qwen-image-edit
.\.venv\Scripts\Activate.ps1
```

**Run script:**
```powershell
python qwen_image_edit_nunchaku.py
```

**Check nunchaku version:**
```powershell
pip show nunchaku  # Should be 1.0.1+torch2.5
```

**Verify model in code:**
```powershell
Get-Content qwen_image_edit_nunchaku.py | Select-String "nunchaku-tech"
```

---

**Made with ‚ù§Ô∏è for AI image editing on consumer GPUs**
