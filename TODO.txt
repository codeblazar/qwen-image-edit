Qwen Image Edit Project - TODO List
====================================

IMPROVEMENTS IDENTIFIED FROM OUTPUT:
====================================

[ ] Install hf_xet for better download performance
    Command: pip install hf_xet
    OR: pip install huggingface_hub[hf_xet]
    Benefit: Faster model downloads from Hugging Face Hub using Xet Storage
    Issue: Warning appears 8 times during download (spammy)

[ ] Suppress/handle repeated "Xet Storage" warnings
    Currently: Same warning printed 8+ times
    Solution: Either install hf_xet or suppress duplicate warnings

[✓] Move generated images to organized folder
    COMPLETED: generated-images/ folder created
    Format: generated-images/output_r128_YYYYMMDD_HHMMSS.png
    Benefit: Project root clean, outputs organized by timestamp

[ ] Handle torch_dtype deprecation warning
    Warning: "`torch_dtype` is deprecated! Use `dtype` instead!"
    Fix: Update code to use `dtype` parameter instead of `torch_dtype`

[ ] Suppress guidance_scale warning for non-guidance-distilled models
    Warning: "guidance_scale is passed as 1.0, but ignored since the model is not guidance-distilled."
    Fix: Either remove guidance_scale=1.0 or suppress this expected warning

[ ] Handle unexpected config attributes warning
    Warning: "The config attributes {'pooled_projection_dim': 768} were passed to NunchakuQwenImageTransformer2DModel, but are not expected"
    Issue: Benign warning from model config mismatch
    Fix: Update nunchaku model config or suppress this warning

[ ] Add progress indicators/cleaner output
    Current: Multiple progress bars and warnings intermixed
    Improvement: Better organized output, maybe logging levels

[✓] Add timestamp to generated image filenames
    COMPLETED: Now uses format output_r128_20251002_085316.png
    Benefit: Never overwrites previous generations
      - Easy to find all outputs
      - Can add to .gitignore
      - Organize by date/prompt
    Implementation:
      - Create folder if not exists
      - Save with timestamp: generated/20251002_085316_r128.png
      - Optionally save prompt as metadata .txt file

====================================
SPEED OPTIMIZATION PLAN:
====================================

[ ] Implement Lightning 8-Step Model for 5x Speed Improvement
    Current: Standard model, 40 steps, ~2:45 generation time
    Target: Lightning model, 8 steps, ~33 seconds generation time
    
    CRITICAL LESSONS FROM PAST ERRORS:
    ⚠️ ALWAYS activate venv first: .\.venv\Scripts\Activate.ps1
    ⚠️ Check (.venv) appears in prompt before ANY command
    ⚠️ Use correct quantized model (nunchaku-tech/...)
    ⚠️ Never use wrong nunchaku package (0.15.4 stats)
    
    STEP-BY-STEP IMPLEMENTATION:
    
    1. Create new script: qwen_image_edit_lightning.py
       - Copy from qwen_image_edit_nunchaku.py
       - Change model path to: svdq-int4_r128-lightningv2.0-8steps
       - Change num_inference_steps from 40 to 8
       - Add "lightning" prefix to output filename
       - Keep all other settings identical
    
    2. Test with venv active:
       - cd C:\Projects\qwen-image-edit
       - .\.venv\Scripts\Activate.ps1
       - Verify (.venv) in prompt
       - python qwen_image_edit_lightning.py
    
    3. Compare results:
       - Time: Standard (~2:45) vs Lightning (~33s)
       - Quality: Visual comparison of outputs
       - VRAM usage: Should be similar (~23GB)
    
    4. Update documentation:
       - Add lightning script to README
       - Document speed vs quality tradeoff
       - Update project structure
    
    5. Optional: Create 4-step ultra-fast variant
       - Only if 8-step quality is acceptable
       - Model: svdq-int4_r128-lightningv2.0-4steps
       - Steps: 4 (10x faster, ~16 seconds)
    
    EXPECTED OUTCOMES:
    ✅ 5x faster generation (2:45 → 33s)
    ✅ Same VRAM usage (~23GB)
    ✅ Same model size (~12.7GB)
    ✅ Acceptable quality (rank 128)
    ✅ Both scripts available (standard for quality, lightning for speed)
    
    VERIFICATION CHECKLIST:
    [ ] New script created
    [ ] Venv activated before test
    [ ] Model downloads successfully
    [ ] Generation completes without errors
    [ ] Speed improvement measured
    [ ] Quality acceptable
    [ ] Documentation updated
    [ ] Changes committed to git

====================================
Completed Tasks:
====================================

[✓] Set up virtual environment (.venv)
[✓] Install PyTorch 2.5.1+cu121 with CUDA support
[✓] Install diffusers 0.36.0.dev0 from GitHub
[✓] Install Visual Studio Build Tools 2022 with C++ components
[✓] Patch PyTorch to bypass CUDA version check (13.0 vs 12.1)
[✓] Compile and install nunchaku from source with git submodules
[✓] Verify nunchaku installation (NunchakuQwenImageTransformer2DModel)
[✓] Successfully run qwen_image_edit_nunchaku.py
[✓] Generated first image successfully (2:44 inference time, 40 steps)

====================================
